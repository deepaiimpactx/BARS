{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd77f45c-f931-4153-9212-7e37f546e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#importing libraries for machine learning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a16c02-a601-46c0-9929-62ab054a7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify(df, n_points):\n",
    "    # Using train_test_split with stratify to select n_points while maintaining class balance\n",
    "    stratified_sample, _ = train_test_split(df, test_size=n_points, stratify=df['Is_Malicious'], random_state=42)\n",
    "    return stratified_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5e1ec4-9205-4105-bcc4-cd5aad062d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataframe(df, class_column):\n",
    "    # Count occurrences of each class\n",
    "    class_counts = df[class_column].value_counts()\n",
    "    \n",
    "    # Majority and minority classes\n",
    "    majority_class = max(class_counts)\n",
    "    minority_class = min(class_counts)\n",
    "    \n",
    "    # Target count for balancing\n",
    "    target_count = int((majority_class + minority_class)/2)\n",
    "    \n",
    "    resampled_dfs = []  # List to store resampled DataFrames\n",
    "    \n",
    "    for class_value, count in class_counts.items():\n",
    "        class_df = df[df[class_column] == class_value]\n",
    "        \n",
    "        if count < target_count:\n",
    "            # Resample with replacement\n",
    "            resampled_df = class_df.sample(n=target_count, replace=True, random_state=42)\n",
    "        elif count > target_count:\n",
    "            # Resample without replacement\n",
    "            resampled_df = class_df.sample(n=target_count, replace=False, random_state=42)\n",
    "        else:\n",
    "            resampled_df = class_df\n",
    "        \n",
    "        resampled_dfs.append(resampled_df)\n",
    "    \n",
    "    # Combine all resampled DataFrames\n",
    "    resampled_df = pd.concat(resampled_dfs)\n",
    "    \n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958a313a-23a8-40de-908d-c9f32a39765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(df, target_column):\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7a1c04-834b-4e2a-a6ea-2a9cf92398e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sensornetguard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cb1ce2-3c87-4472-b6f5-efdb5e4aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop( ['IP_Address','Node_ID','Timestamp'], axis=1) #[,'Pinged_IP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f5450a-48e1-440d-a3ef-02d0bb76f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Is_Malicious'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabca628-f7c3-4dd1-afbd-edbbbca9434f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = stratify(df, 5000)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c41b127-9f5c-4190-b74a-0eb46da806da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = resample_dataframe(df, target_column)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d800b004-aae1-4909-bd6b-6f59889396c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled, y, feature_names = preprocess_data(df, target_column)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f79c7-e446-41ac-ab56-f28e4c15909d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681345a2-c036-4a09-a4c2-fb9ae03400c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create the LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=32))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a845c0-3417-4aca-9368-113990a66cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the DNN model\n",
    "def create_dnn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff342dc-5bb4-435e-a1d4-b6077d853311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Feature Selection\n",
    "def dnn_feature_selection(X_train, y_train, feature_names, num_features=5):\n",
    "    # Train a RandomForestClassifier to get feature importances\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importances and sort them\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Get top 'num_features' features\n",
    "    selected_features = [feature_names[i] for i in sorted_idx[:num_features]]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d707c9c6-c316-48a3-b95d-5d5f70c43114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Feature Selection\n",
    "def lstm_feature_selection(X_train, y_train, X_valid, feature_names, num_features=5):\n",
    "    time_steps = 1\n",
    "    features = X_train.shape[1]\n",
    "    input_shape = (time_steps, features)\n",
    "    \n",
    "    # Reshape data for LSTM\n",
    "    X_train_lstm = np.reshape(X_train, (X_train.shape[0], time_steps, features))\n",
    "    X_valid_lstm = np.reshape(X_valid, (X_valid.shape[0], time_steps, features))\n",
    "    \n",
    "    # Train LSTM model\n",
    "    lstm_model = create_lstm_model(input_shape)\n",
    "    lstm_model.fit(X_train_lstm, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Use the LSTM layer outputs to rank features (e.g., use layer's weights)\n",
    "    lstm_weights = lstm_model.layers[0].get_weights()[0]\n",
    "    \n",
    "    # Get the absolute sum of the weights for each feature\n",
    "    feature_importance = np.abs(lstm_weights).sum(axis=1)\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    # Get top 'num_features' features\n",
    "    selected_features = [feature_names[i] for i in sorted_idx[:num_features]]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db86523-a010-4d2c-8525-62a727bdbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\GEN\\ai\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\aravi\\GEN\\ai\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "X_scaled, y, feature_names = preprocess_data(df, target_column)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the time taken for DNN model\n",
    "start_time = time.time()\n",
    "dnn_model = create_dnn_model(X_train.shape[1])\n",
    "dnn_model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "dnn_time = time.time() - start_time\n",
    "\n",
    "# Get the time taken for LSTM model\n",
    "time_steps = 1\n",
    "features = X_train.shape[1]\n",
    "input_shape = (time_steps, features)\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], time_steps, features))\n",
    "X_valid_lstm = np.reshape(X_valid, (X_valid.shape[0], time_steps, features))\n",
    "\n",
    "start_time = time.time()\n",
    "lstm_model = create_lstm_model(input_shape)\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "lstm_time = time.time() - start_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91290273-ea68-4bdf-9704-fc9df6fff47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN finished first, using DNN selected features: ['Data_Throughput', 'Error_Rate', 'Energy_Consumption_Rate', 'Packet_Drop_Rate']\n"
     ]
    }
   ],
   "source": [
    "# Select features based on the model that finishes first\n",
    "if dnn_time < lstm_time:\n",
    "    selected_features = dnn_feature_selection(pd.DataFrame(X_train), y_train, feature_names, num_features=4)\n",
    "    print(\"DNN finished first, using DNN selected features:\", selected_features)\n",
    "else:\n",
    "    selected_features = lstm_feature_selection(pd.DataFrame(X_train), y_train, X_valid, feature_names, num_features=4)\n",
    "    print(\"LSTM finished first, using LSTM selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "004ac88f-cf54-41e6-a47b-263ce702bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data_Throughput', 'Error_Rate', 'Energy_Consumption_Rate', 'Packet_Drop_Rate']\n"
     ]
    }
   ],
   "source": [
    "feature_names=selected_features\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d8c774-c5aa-43fc-b5cc-d55b1ebef7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\GEN\\ai\\Lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\aravi\\AppData\\Local\\Temp\\ipykernel_19128\\2328207575.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, metrics_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2000, number of negative: 2000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1006\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "               Model  Accuracy  Precision  Recall  F1-score  ROC AUC  \\\n",
      "0      Random Forest     1.000   1.000000     1.0  1.000000    1.000   \n",
      "1            XGBoost     1.000   1.000000     1.0  1.000000    1.000   \n",
      "2           LightGBM     1.000   1.000000     1.0  1.000000    1.000   \n",
      "3        Extra Trees     1.000   1.000000     1.0  1.000000    1.000   \n",
      "4  Gradient Boosting     0.999   0.998004     1.0  0.999001    0.999   \n",
      "\n",
      "   Training Time  Prediction Time  \n",
      "0       0.440681         0.011351  \n",
      "1       0.083795         0.000999  \n",
      "2       0.498563         0.003000  \n",
      "3       0.145587         0.011626  \n",
      "4       0.867545         0.002026  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df[target_column]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features using Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_selected_scaled = scaler.fit_transform(X_train)\n",
    "X_test_selected_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Defining classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Creating an empty DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Training Time', 'Prediction Time'])\n",
    "\n",
    "# Loop through classifiers\n",
    "for model_name, classifier in classifiers.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training the classifier\n",
    "    classifier.fit(X_train_selected_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Making predictions on the test set\n",
    "    predictions = classifier.predict(X_test_selected_scaled)\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # Calculating metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    # Constructing a DataFrame row with the current metrics\n",
    "    metrics_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1-score': [f1],\n",
    "        'ROC AUC': [roc_auc],\n",
    "        'Training Time': [training_time],\n",
    "        'Prediction Time': [prediction_time]\n",
    "    })\n",
    "\n",
    "    # Concatenating the current metrics with the main metrics DataFrame\n",
    "    metrics_df = pd.concat([metrics_df, metrics_row], ignore_index=True)\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d485076-fc44-42cb-bc6b-a9e768b22235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
